{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noisy(mnist_image):\n",
    "    mnist_rand_image = np.copy(mnist_image)\n",
    "\n",
    "    for x in np.nditer(mnist_rand_image, op_flags=['readwrite']):\n",
    "        randval = np.random.randint(0,10)\n",
    "        if randval == 0:\n",
    "            x[...] = 0\n",
    "        elif randval == 1:\n",
    "            x[...] = 255\n",
    "            \n",
    "    return mnist_rand_image\n",
    "\n",
    "#plt.imshow(make_noisy(mnist_data[0][0][0]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_noisy_array = []\n",
    "train_data_size = mnist_data[0][0].shape[0]\n",
    "\n",
    "for i in range(0,train_data_size):\n",
    "    train_noisy_array.append(make_noisy(mnist_data[0][0][i]))\n",
    "\n",
    "train_noisy_array = np.asarray(train_noisy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_noisy_array = []\n",
    "test_data_size = mnist_data[1][0].shape[0]\n",
    "\n",
    "for i in range(0,test_data_size):\n",
    "    test_noisy_array.append(make_noisy(mnist_data[1][0][i]))\n",
    "\n",
    "test_noisy_array = np.asarray(test_noisy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=mnist_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels=mnist_data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_mnist = ((train_noisy_array, train_labels),(test_noisy_array, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.3553 - acc: 0.8993 - val_loss: 0.1968 - val_acc: 0.9486\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.1707 - acc: 0.9519 - val_loss: 0.1488 - val_acc: 0.9597\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.1227 - acc: 0.9657 - val_loss: 0.1148 - val_acc: 0.9683\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0946 - acc: 0.9735 - val_loss: 0.1057 - val_acc: 0.9698\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0772 - acc: 0.9790 - val_loss: 0.0968 - val_acc: 0.9726\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0646 - acc: 0.9826 - val_loss: 0.0925 - val_acc: 0.9744\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - ETA: 0s - loss: 0.0546 - acc: 0.985 - 3s 52us/step - loss: 0.0546 - acc: 0.9855 - val_loss: 0.0857 - val_acc: 0.9753\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.0459 - acc: 0.9883 - val_loss: 0.0800 - val_acc: 0.9759\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0400 - acc: 0.9901 - val_loss: 0.0795 - val_acc: 0.9755\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0343 - acc: 0.9916 - val_loss: 0.0777 - val_acc: 0.9768\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0295 - acc: 0.9931 - val_loss: 0.0763 - val_acc: 0.9767\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0254 - acc: 0.9950 - val_loss: 0.0723 - val_acc: 0.9782\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.0224 - acc: 0.9955 - val_loss: 0.0726 - val_acc: 0.9785\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0198 - acc: 0.9965 - val_loss: 0.0736 - val_acc: 0.9780\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0174 - acc: 0.9975 - val_loss: 0.0716 - val_acc: 0.9791\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0153 - acc: 0.9979 - val_loss: 0.0713 - val_acc: 0.9788\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.0134 - acc: 0.9982 - val_loss: 0.0708 - val_acc: 0.9795\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0121 - acc: 0.9987 - val_loss: 0.0721 - val_acc: 0.9782\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.0110 - acc: 0.9988 - val_loss: 0.0717 - val_acc: 0.9788\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.0099 - acc: 0.9992 - val_loss: 0.0713 - val_acc: 0.9796\n",
      "Test loss: 0.06225668139125919\n",
      "Test accuracy: 0.9801\n"
     ]
    }
   ],
   "source": [
    "## CLEAN MNIST\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() \n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.03, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save(\"keras_mnist_modelv2_one_dense_clean.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.5350 - acc: 0.8367 - val_loss: 0.3787 - val_acc: 0.8822\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.2959 - acc: 0.9105 - val_loss: 0.2997 - val_acc: 0.9067\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2019 - acc: 0.9409 - val_loss: 0.2564 - val_acc: 0.9199\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.1403 - acc: 0.9611 - val_loss: 0.2393 - val_acc: 0.9258\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0978 - acc: 0.9757 - val_loss: 0.2319 - val_acc: 0.9298\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0668 - acc: 0.9855 - val_loss: 0.2293 - val_acc: 0.9308\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0447 - acc: 0.9933 - val_loss: 0.2281 - val_acc: 0.9322\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0300 - acc: 0.9974 - val_loss: 0.2267 - val_acc: 0.9338\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0209 - acc: 0.9992 - val_loss: 0.2271 - val_acc: 0.9349\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0152 - acc: 0.9999 - val_loss: 0.2304 - val_acc: 0.9354\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.0118 - acc: 0.9999 - val_loss: 0.2317 - val_acc: 0.9351\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.2362 - val_acc: 0.9345\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.2388 - val_acc: 0.9353\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.2400 - val_acc: 0.9360\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.2416 - val_acc: 0.9365\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.2443 - val_acc: 0.9361\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.2455 - val_acc: 0.9366\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.2466 - val_acc: 0.9364\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.2481 - val_acc: 0.9371\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.2496 - val_acc: 0.9370\n",
      "Test loss: 0.2283195750296698\n",
      "Test accuracy: 0.9411\n"
     ]
    }
   ],
   "source": [
    "## NOISY MNIST\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = noisy_mnist\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.03, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save(\"keras_mnist_modelv2_one_dense_noisy.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleanup_for_evaluation_helper(xx_test, yy_test, numnum_classes=10):\n",
    "    num_classes = numnum_classes\n",
    "    x_test = xx_test\n",
    "    y_test = yy_test\n",
    "    \n",
    "    x_test = x_test.reshape(10000, 784)\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_test /= 255\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    return (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_model_filename = \"keras_mnist_modelv2_one_dense_noisy.h5\"\n",
    "noisy_model = load_model(noisy_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 32us/step\n",
      "('Test loss:', 0.15027683409203274)\n",
      "('Test accuracy:', 0.9667)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist_data\n",
    "\n",
    "(x_test, y_test) = data_cleanup_for_evaluation_helper(x_test, y_test)\n",
    "\n",
    "score = noisy_model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 24us/step\n",
      "('Test loss:', 0.23684982892246917)\n",
      "('Test accuracy:', 0.9391)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = noisy_mnist\n",
    "\n",
    "(x_test, y_test) = data_cleanup_for_evaluation_helper(x_test, y_test)\n",
    "\n",
    "score = noisy_model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_model_filename = \"keras_mnist_modelv2_one_dense_clean.h5\"\n",
    "clean_model = load_model(clean_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 27us/step\n",
      "('Test loss:', 0.06225668139125919)\n",
      "('Test accuracy:', 0.9801)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist_data\n",
    "\n",
    "(x_test, y_test) = data_cleanup_for_evaluation_helper(x_test, y_test)\n",
    "\n",
    "score = clean_model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 26us/step\n",
      "('Test loss:', 0.5437456713080406)\n",
      "('Test accuracy:', 0.8279)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = noisy_mnist\n",
    "\n",
    "(x_test, y_test) = data_cleanup_for_evaluation_helper(x_test, y_test)\n",
    "\n",
    "score = clean_model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BELOW WILL BE NOISY THEN CLEAN TRAINING\n",
    "\n",
    "train_noisy_clean_array = []\n",
    "train_data_size = mnist_data[0][0].shape[0]\n",
    "validation_split_size = 0.2\n",
    "actual_train_data_size = int(train_data_size * (1 - validation_split_size))\n",
    "\n",
    "# Training set\n",
    "for i in range(0, actual_train_data_size):\n",
    "    if (i < actual_train_data_size/2):\n",
    "        train_noisy_clean_array.append(make_noisy(mnist_data[0][0][i]))\n",
    "    else:\n",
    "        train_noisy_clean_array.append(mnist_data[0][0][i])\n",
    "\n",
    "# Validation set \n",
    "validation_data_size = train_data_size - actual_train_data_size\n",
    "\n",
    "for i in range(0, validation_data_size):\n",
    "    if (i < validation_data_size/2):\n",
    "        train_noisy_clean_array.append(make_noisy(mnist_data[0][0][i+actual_train_data_size]))\n",
    "    else:\n",
    "        train_noisy_clean_array.append(mnist_data[0][0][i+actual_train_data_size])\n",
    "\n",
    "train_noisy_clean_array = np.asarray(train_noisy_clean_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=mnist_data[0][1]\n",
    "\n",
    "test_labels=mnist_data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_then_clean_mnist = ((train_noisy_clean_array,train_labels),(mnist_data[1][0],test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.4580 - acc: 0.8652 - val_loss: 0.2961 - val_acc: 0.9110\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2339 - acc: 0.9325 - val_loss: 0.2245 - val_acc: 0.9338\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.1589 - acc: 0.9556 - val_loss: 0.2017 - val_acc: 0.9385\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.1156 - acc: 0.9690 - val_loss: 0.1772 - val_acc: 0.9453\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0856 - acc: 0.9794 - val_loss: 0.1736 - val_acc: 0.9450\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0657 - acc: 0.9858 - val_loss: 0.1628 - val_acc: 0.9486\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0518 - acc: 0.9892 - val_loss: 0.1619 - val_acc: 0.9499\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0417 - acc: 0.9918 - val_loss: 0.1575 - val_acc: 0.9508\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0341 - acc: 0.9937 - val_loss: 0.1637 - val_acc: 0.9501\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0286 - acc: 0.9951 - val_loss: 0.1637 - val_acc: 0.9506\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.0245 - acc: 0.9959 - val_loss: 0.1599 - val_acc: 0.9517\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.0208 - acc: 0.9967 - val_loss: 0.1600 - val_acc: 0.9533\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.0179 - acc: 0.9974 - val_loss: 0.1644 - val_acc: 0.9518\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.0157 - acc: 0.9978 - val_loss: 0.1599 - val_acc: 0.9540\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.0138 - acc: 0.9981 - val_loss: 0.1607 - val_acc: 0.9537\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0122 - acc: 0.9985 - val_loss: 0.1621 - val_acc: 0.9539\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.0106 - acc: 0.9989 - val_loss: 0.1657 - val_acc: 0.9525\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0095 - acc: 0.9990 - val_loss: 0.1661 - val_acc: 0.9515\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.0086 - acc: 0.9992 - val_loss: 0.1685 - val_acc: 0.9518\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.0076 - acc: 0.9995 - val_loss: 0.1693 - val_acc: 0.9523\n",
      "Test loss: 0.078595794655988\n",
      "Test accuracy: 0.9774\n"
     ]
    }
   ],
   "source": [
    "## NOISY_THEN_CLEAN MNIST, BUT BECAUSE OF SHUFFLING, THE NOISY AND CLEAN WILL BE UNIFORMLY INTERSPERSED\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = noisy_then_clean_mnist #mnist.load_data() \n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.03, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save(\"keras_mnist_modelv2_one_dense_noisy_clean_uniform.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 24us/step\n",
      "('Test loss:', 0.078595794655988)\n",
      "('Test accuracy:', 0.9774)\n"
     ]
    }
   ],
   "source": [
    "noisy_clean_uniform_model_filename = \"keras_mnist_modelv2_one_dense_noisy_clean_uniform.h5\"\n",
    "noisy_clean_uniform_model = load_model(noisy_clean_uniform_model_filename)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist_data\n",
    "\n",
    "(x_test, y_test) = data_cleanup_for_evaluation_helper(x_test, y_test)\n",
    "\n",
    "score = noisy_clean_uniform_model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 31us/step\n",
      "('Test loss:', 0.23047032042229548)\n",
      "('Test accuracy:', 0.9363)\n"
     ]
    }
   ],
   "source": [
    "noisy_clean_uniform_model_filename = \"keras_mnist_modelv2_one_dense_noisy_clean_uniform.h5\"\n",
    "noisy_clean_uniform_model = load_model(noisy_clean_uniform_model_filename)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = noisy_mnist\n",
    "\n",
    "(x_test, y_test) = data_cleanup_for_evaluation_helper(x_test, y_test)\n",
    "\n",
    "score = noisy_clean_uniform_model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#A network trained on clean mnist (i.e. regular mnist) has the following performance:\n",
    "    ##regular mnist test accuracy: 0.9801\n",
    "    ##noisy mnist test accuracy  : 0.8355\n",
    "\n",
    "#A network trained on all noisy mnist has the following performance:\n",
    "    ##regular mnist test accuracy: 0.9667\n",
    "    ##noisy mnist test accuracy  : 0.9411\n",
    "    \n",
    "#A network train on noisy and clean mnist (half for each), uniformly dispersed in the data, has the following performance:\n",
    "    ##regular mnist test accuracy: 0.9774\n",
    "    ##noisy mnist test accuracy  : 0.9368\n",
    "    \n",
    "##For the noisy mnist test accuracy, there can be descrepancy because adding noise is random (i.e. two noisy mnist data could be different, as this is something we have not currently accounted for)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
